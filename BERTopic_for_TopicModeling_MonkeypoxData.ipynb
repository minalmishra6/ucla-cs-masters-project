{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **BERTopic for Topic Modeling Monkeypox Twitter Discourse**\n",
        "By Minal Mishra"
      ],
      "metadata": {
        "id": "hpmhXb_n4lth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-Q1mqzxVsRk"
      },
      "outputs": [],
      "source": [
        "# installing libraries\n",
        "!pip install bertopic\n",
        "#!pip install joblib==1.1.0 # uncomment for second and sebsequent runs\n",
        "!pip install cleantext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing modules\n",
        "#!pip install --upgrade joblib==1.1.0   # uncomment for second and sebsequent runs\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic"
      ],
      "metadata": {
        "id": "THKPTiYtVzF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "lpLEgMxmzFti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "import re\n",
        "def clean_data(line):\n",
        "  line = line.strip('\"')  # removes leading and trailing double quotes\n",
        "  line = re.sub(r'\\s+&amp[^A-Za-z0-9]', ' ', line) # removes &amp\n",
        "  line = re.sub(r\"@[A-Za-z0-9_]+\",\"\", line) # removes name mentions\n",
        "  line = re.sub(r\"#[A-Za-z0-9_]+\",\"\", line) # removes hashtags\n",
        "  line = re.sub(r'http\\S+', '', line) # removes hyperlinks\n",
        "  line = re.sub(r'\\\\n', ' ', line) # removes newline characters\n",
        "  line = re.sub(r'[\"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\" \"\\U0001F1E0-\\U0001F1FF\" \"\\U000024C2-\\U0001F251\" \"\\U00002700-\\U000027BF\" \"\\U00002600-\\U000026FF\" \"\\U0001F900-\\U0001F9FF\" \"\\U0001FA70-\\U0001FAFF\"]+',' ', line)  # removes  emojis and pictographs\n",
        "  line = line.split()\n",
        "  line = \" \".join(word for word in line)\n",
        "  line = line.strip()\n",
        "\n",
        "  return line\n",
        "  \n",
        "fr = open(\"/content/40k_all_tweets.txt\")\n",
        "\n",
        "tweets=[]\n",
        "for line in fr:\n",
        "  if not line.isspace():\n",
        "    tweets.append(clean_data(line))\n",
        "tweets=[str for str in tweets if str]\n",
        "\n",
        "print(tweets)\n",
        "\n",
        "fr.close()\n"
      ],
      "metadata": {
        "id": "NSqN8rrmV1zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction and Clustering Models"
      ],
      "metadata": {
        "id": "M3s3A4D4zW1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#UMAP Model for Dimensionality Reduction\n",
        "from umap import UMAP\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0, random_state=42)\n",
        "\n",
        "#HDBSCAN Model for Clustering\n",
        "from hdbscan import HDBSCAN\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=10, prediction_data=True, gen_min_span_tree=False)"
      ],
      "metadata": {
        "id": "4H5vf6YlV39-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = list(stopwords.words('english')) + ['wtf','shit','nit','fuck','lol','oh']\n",
        "\n",
        "#Incorporating biagrams\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1,2), stop_words=stopwords)"
      ],
      "metadata": {
        "id": "L4u4qoSCV6kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Model"
      ],
      "metadata": {
        "id": "gEVse6o40HuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding model - all-MiniLM-L6-v2\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "fd4vLgjjV8gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Application of BERTopic Topic Model**"
      ],
      "metadata": {
        "id": "SBC6cfEG0evt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying LDA model\n",
        "\n",
        "nr_topics=50\n",
        "model = BERTopic(umap_model=umap_model, \n",
        "                 hdbscan_model=hdbscan_model,\n",
        "                 embedding_model=embedding_model, \n",
        "                 vectorizer_model=vectorizer_model, \n",
        "                 top_n_words=10,\n",
        "                 n_gram_range=(1,2),\n",
        "                 min_topic_size=10,\n",
        "                 nr_topics=nr_topics,\n",
        "                 language='english', \n",
        "                 calculate_probabilities='False',\n",
        "                 verbose='True')\n",
        "\n",
        "\n",
        "topics, probs = model.fit_transform(tweets)"
      ],
      "metadata": {
        "id": "4EPBBLb6WAFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# listing documents and topic ids\n",
        "for i in range(50):\n",
        "  print(topics[i],\":\",tweets[i])"
      ],
      "metadata": {
        "id": "g53BdskeWDFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topic representation\n",
        "print(\"Topic Information:\")\n",
        "topic_information = model.get_topic_info()  # returns topic information including - topic id, number of documents that the topic occurs in and name of the topic\n",
        "\n",
        "no_of_topics = len(topic_information)-1 \n",
        "print(topic_information)\n",
        "\n",
        "print(\"\\nNo. of topics:\")\n",
        "print(no_of_topics)\n",
        "\n",
        "no_of_topics=len(model.get_topics())-1\n",
        "print(no_of_topics)\n",
        "\n",
        "# generated topics\n",
        "model_topics=model.get_topics()\n",
        "print(\"\\n\\n Topics and words per topic with c-TFIDF scores for every word:\",model_topics)\n",
        "topics_org=[]\n",
        "for i in model_topics:\n",
        "  if i == -1:\n",
        "    continue\n",
        "  topic= model.get_topic(i)\n",
        "  words_in_topic_org=[]\n",
        "  for word in topic:\n",
        "    words_in_topic_org.append(word[0])\n",
        "  topics_org.append(words_in_topic_org)\n",
        "\n",
        "print(\"\\n\\n Topics and words per topic:\\n\", topics_org)\n",
        "  \n",
        "\n",
        "\n",
        "# topics for evaluation\n",
        "print(\"\\n\\n Topics and words per topic (For Evaluation):\")\n",
        "topics=[]\n",
        "for i in model_topics:\n",
        "  if i == -1:\n",
        "    continue\n",
        "  words_in_topic=[]\n",
        "  topic= model.get_topic(i)\n",
        "  for word in topic:\n",
        "    if len(word[0].split()) == 1:\n",
        "        english_word = re.sub(r\"[^a-z0-9]\",\" \", word[0])\n",
        "        if len(english_word.replace(\" \",\"\").strip()) == len(word[0].strip()):\n",
        "          words_in_topic.append(english_word)\n",
        "    else:\n",
        "      for sub_word in word[0].split():\n",
        "        english_word = re.sub(r\"[^a-z0-9]\",\" \", sub_word)\n",
        "        if len(english_word.replace(\" \",\"\").strip()) == len(sub_word.strip()):\n",
        "          words_in_topic.append(english_word)\n",
        "  if len(words_in_topic) != 0:\n",
        "    topics.append(words_in_topic)\n",
        "\n",
        "print(topics)\n",
        "        "
      ],
      "metadata": {
        "id": "x7TeDoN2WJhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "Neg_jnQM35jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the topics using topic coherence\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# creating BoW corpus for evaluation\n",
        "def clean_data_for_eval(line):\n",
        "  line = line.lower()   # convert all characters to lower case\n",
        "  line = re.sub(r'[\\'\"]', \" \", line)  # to avoid removing contractions in english\n",
        "  line = re.sub(r'\\s+&amp[^A-Za-z0-9]', ' ', line) # removes  &amp\n",
        "  line = re.sub(r\"@[A-Za-z0-9_]+\",\"\", line) # removes name mentions\n",
        "  line = re.sub(r\"#[A-Za-z0-9_]+\",\"\", line) # remove hashtags\n",
        "  line = re.sub(r'http\\S+', '', line) # removes hyperlinks\n",
        "  line = re.sub(r'\\\\n', ' ', line) # remove newline characters\n",
        "  line = re.sub(r'[()!*?\\[\\]]', ' ', line)# remove punctuation \n",
        "  line = re.sub(r\"[^a-z0-9]\",\" \", line) # removes all characters except [a-z] and [0-9]\n",
        "  line = line.split()\n",
        "  line = [w for w in line if not w in stopwords]\n",
        "  line = \" \".join(word for word in line)\n",
        "  line = line.strip()\n",
        "\n",
        "  return line\n",
        "  \n",
        "fr_eval = open(\"/content/40k_all_tweets.txt\")\n",
        "\n",
        "tweets=[]\n",
        "for line in fr_eval:\n",
        "  if not line.isspace():\n",
        "    tweets.append(clean_data_for_eval(line))\n",
        "tweets=[str for str in tweets if str]\n",
        "print(tweets)\n",
        "\n",
        "data = tweets\n",
        "\n",
        "texts = [[word for word in str(document).split()] for document in tweets]\n",
        "print(texts)\n",
        "id2word = corpora.Dictionary(texts)\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "fr_eval.close()\n",
        "\n",
        "coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=id2word, coherence='c_v')\n",
        "coherence = coherence_model.get_coherence()\n",
        "print(\"\\nCoherence Score:\",coherence)"
      ],
      "metadata": {
        "id": "wdE4EcRtWL5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualization of Topics**"
      ],
      "metadata": {
        "id": "n00vuYgM3--J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing topics\n",
        "\n",
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "xNBe7Ho3WO8r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}