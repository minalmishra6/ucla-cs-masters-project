{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Latent Dirichlet Allocation (LDA) for Topic Modeling Monkeypox Twitter Discourse**\n",
        "By Minal Mishra"
      ],
      "metadata": {
        "id": "zQ0bcEFDmaMo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRcbwSgmb309"
      },
      "outputs": [],
      "source": [
        "# installing libraries\n",
        "!python -m pip install nltk\n",
        "!python -m pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNRt9LPmb8IO"
      },
      "outputs": [],
      "source": [
        "# importing modules\n",
        "import numpy as np\n",
        "import json\n",
        "import glob\n",
        "\n",
        "#gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "#spacy and nltk\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading and Preprocessing"
      ],
      "metadata": {
        "id": "5mYdXasZvHoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q09fzZVsjLI-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean_data(line):\n",
        "  line = line.lower()   # convert all characters to lower case\n",
        "  line = re.sub(r'[\\'\"]', \" \", line) # to avoid removing contractions in english\n",
        "  line = re.sub(r'\\s+&amp[^A-Za-z0-9]', ' ', line) # removes &amp\n",
        "  line = re.sub(r\"@[A-Za-z0-9_]+\",\"\", line) # removes name mentions\n",
        "  line = re.sub(r\"#[A-Za-z0-9_]+\",\"\", line) # removes hashtags\n",
        "  line = re.sub(r'http\\S+', '', line) # removes all hyperlinks\n",
        "  line = re.sub(r'\\\\n', ' ', line) # remove newline characters\n",
        "  line = re.sub(r'[()!*?\\[\\]]', ' ', line) # remove punctuation marks\n",
        "  line = re.sub(r\"[^a-z0-9]\",\" \", line) # removes all characters except [a-z] and [0-9]\n",
        "  line = line.split()\n",
        "  line = [w for w in line if not w in stopwords]\n",
        "  line = \" \".join(word for word in line)\n",
        "  line = line.strip()\n",
        "\n",
        "  return line\n",
        "\n",
        "fr = open(\"40k_all_tweets.txt\") # replace 40k_all_tweets.txt with your dataset file name before execution\n",
        "\n",
        "tweets=[]\n",
        "for line in fr:\n",
        "  if not line.isspace():\n",
        "    tweets.append(clean_data(line))\n",
        "tweets=[str for str in tweets if str]\n",
        "\n",
        "print(tweets)\n",
        "\n",
        "data = tweets\n",
        "fr.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHz7YJN-l07n"
      },
      "outputs": [],
      "source": [
        "#lemmatisation\n",
        "def lemmatization(texts, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"PROPN\", \"ADV\"]):\n",
        "  nlp = spacy.load(\"en_core_web_sm\", disable=['parser','ner'])\n",
        "  texts_out=[]\n",
        "\n",
        "  for text in texts:\n",
        "    doc = nlp(text)\n",
        "    new_text=[]\n",
        "    for token in doc:\n",
        "      if token.pos_ in allowed_postags and not token.is_stop:\n",
        "        new_text.append(token.lemma_)\n",
        "    final = \" \".join(new_text)\n",
        "    texts_out.append(final)\n",
        "  return texts_out\n",
        "\n",
        "lemmatized_texts=lemmatization(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw_bI-KprBio"
      },
      "outputs": [],
      "source": [
        "#removing stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "new_lemmatized_texts = []\n",
        "for sentence in lemmatized_texts:\n",
        "  new_lemmatized_texts.append(remove_stopwords(sentence))\n",
        "\n",
        "lemmatized_texts = new_lemmatized_texts\n",
        "\n",
        "\n",
        "# prerocessing text\n",
        "def gen_words(texts):\n",
        "  final = []\n",
        "  for text in texts:\n",
        "    new = gensim.utils.simple_preprocess(text, deacc= True, min_len= 3)\n",
        "    final.append(new)\n",
        "  return final\n",
        "\n",
        "data_words=gen_words(lemmatized_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH3kVMx4yWI6"
      },
      "outputs": [],
      "source": [
        "# removing custom stopwords\n",
        "\n",
        "custom_stopwords = ['wtf','shit','nit','fuck','lol','oh']\n",
        "\n",
        "new_data_words=[]\n",
        "for sentence in data_words:\n",
        "   new_sentence = [word for word in sentence if word not in custom_stopwords]\n",
        "   new_data_words.append(new_sentence)\n",
        "\n",
        "data_words = new_data_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CMFVPecjF0g"
      },
      "outputs": [],
      "source": [
        "# Adding Bigrams to Vocabulary\n",
        "\n",
        "from gensim.models import Phrases\n",
        "\n",
        "bigrams = Phrases(data_words, min_count = 5)         \n",
        "\n",
        "for sentence_id in range(len(data_words)):\n",
        "  for token in bigrams[data_words[sentence_id]]:\n",
        "    if '_' in token:\n",
        "      data_words[sentence_id].append(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating TFIDF Embeddings"
      ],
      "metadata": {
        "id": "59RkeRsDwIvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PX5nLY98eYq"
      },
      "outputs": [],
      "source": [
        "# Creating the tfidf word embeddings for corpus of tweets\n",
        "\n",
        "from gensim import corpora\n",
        "\n",
        "#creating the vocabulary\n",
        "dct = corpora.Dictionary(data_words)\n",
        "\n",
        "#BoW embedding/model\n",
        "BoW_corpus = [dct.doc2bow(doc) for doc in data_words]\n",
        "print(\"Bow Corpus:\", BoW_corpus)\n",
        "\n",
        "#tfidf embedding/model\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "#tfidf_corpus = TfidfModel(BoW_corpus)\n",
        "tfidf_corpus = TfidfModel(BoW_corpus, normalize=True, pivot=10, slope=0.25)\n",
        "\n",
        "BoW_corpus = [i for i in BoW_corpus if i != []]\n",
        "tfidf_corpus_final = tfidf_corpus[BoW_corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Application of LDA Topic Model**"
      ],
      "metadata": {
        "id": "FcV6Eb3XwRwm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGDEZjBsMQmc"
      },
      "outputs": [],
      "source": [
        "# applying the LDA model\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "num_topics=16\n",
        "lda_model = LdaModel(tfidf_corpus_final, num_topics = num_topics, id2word=dct, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, eval_every=10, iterations=50, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1Q9liXAC8_x"
      },
      "outputs": [],
      "source": [
        "print(\"Topic Word Distribution (word_probability*word pairs per topic):\")\n",
        "print(lda_model.get_topics())      # probabilities of each word in each topic, for all topics\n",
        "\n",
        "#Creating topics (list of listt of words) for evaluation\n",
        "topics_org=[]\n",
        "for topicid in range(num_topics):\n",
        "  topics_org.append([dct.id2token[word[0]] for word in lda_model.get_topic_terms(topicid, topn=10)])  # gives word_id, probability pairs in descending order of relevancy\n",
        "print(\"\\nTopics:\\n\",topics_org)\n",
        "\n",
        "topics=[]\n",
        "for words_per_topic in topics_org:\n",
        "  topic=[]\n",
        "  for word in words_per_topic:\n",
        "    word_split=word.split('_')\n",
        "    if len(word_split) == 1:\n",
        "      topic.append(word)\n",
        "    else:\n",
        "      [topic.append(subword) for subword in word_split]\n",
        "  topics.append(topic)\n",
        "\n",
        "print(\"\\nTopics:\\n\",topics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "SmPLxVmsxCml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5Zb8RFY_hLI"
      },
      "outputs": [],
      "source": [
        "# Evaluating the topics - Topic Coherence\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "\n",
        "#creating BoW corpus for evaluation\n",
        "nlp_eval = spacy.load(\"en_core_web_sm\", disable=['parser','ner'])\n",
        "\n",
        "def clean_data_for_eval(line):\n",
        "  line = line.lower()   # convert all characters to lower case\n",
        "  line = re.sub(r'[\\'\"]', \" \", line) # to avoid removing contractions in english\n",
        "  line = re.sub(r'\\s+&amp[^A-Za-z0-9]', ' ', line) # removes  &amp\n",
        "  line = re.sub(r\"@[A-Za-z0-9_]+\",\"\", line) # removes name mentions\n",
        "  line = re.sub(r\"#[A-Za-z0-9_]+\",\"\", line)  # remove hashtags\n",
        "  line = re.sub(r'http\\S+', '', line) # removes hyperlinks\n",
        "  line = re.sub(r'\\\\n', ' ', line) # removes newline characters\n",
        "  line = re.sub(r'[()!*?\\[\\]]', ' ', line) # removes punctuation\n",
        "  line = re.sub(r\"[^a-z0-9]\",\" \", line) # removes all characters except [a-z] and [0-9]\n",
        "  line = line.split()\n",
        "  line = [w for w in line if not w in stopwords]\n",
        "  line = \" \".join(word for word in line)\n",
        "  line = line.strip()\n",
        "  doc = nlp_eval(line)\n",
        "  new_line=[]\n",
        "\n",
        "  for token in doc:\n",
        "    new_line.append(token.lemma_)\n",
        "  line = \" \".join(new_line)\n",
        "\n",
        "  return line\n",
        "\n",
        "fr_eval = open(\"/content/40k_all_tweets.txt\")\n",
        "\n",
        "tweets=[]\n",
        "for line in fr_eval:\n",
        "  if not line.isspace():\n",
        "    tweets.append(clean_data_for_eval(line))\n",
        "tweets=[str for str in tweets if str]\n",
        "print(tweets)\n",
        "\n",
        "data = tweets\n",
        "\n",
        "texts = [[word for word in str(document).split()] for document in tweets]\n",
        "print(texts)\n",
        "id2word = corpora.Dictionary(texts)\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "fr_eval.close()\n",
        "\n",
        "coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=id2word, coherence='c_v')\n",
        "coherence = coherence_model.get_coherence()\n",
        "print(\"\\nCoherence Score:\",coherence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualization of Topics**"
      ],
      "metadata": {
        "id": "ycC53ZwfwkDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKsKVrCVM8e6"
      },
      "outputs": [],
      "source": [
        "# Vizualization of LDA Model\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "viz = pyLDAvis.gensim_models.prepare(lda_model, tfidf_corpus_final, dct)\n",
        "\n",
        "viz\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}